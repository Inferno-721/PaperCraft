{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3d3f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import requests\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from open_deep_research.graph import builder as odr_builder\n",
    "import uuid\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff9dfa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment variables\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize memory for checkpointing\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e3341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search function using Tavily\n",
    "def get_search_results(query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
    "    tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY is not set\")\n",
    "    search_url = \"https://api.tavily.com/search\"\n",
    "    search_params = {\n",
    "        \"api_key\": tavily_api_key,\n",
    "        \"query\": query,\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    response = requests.post(search_url, json=search_params)\n",
    "    response.raise_for_status()\n",
    "    results = response.json().get(\"results\", [])\n",
    "    return [{\"title\": r.get(\"title\", \"\"), \"content\": r.get(\"content\", \"\")} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e748de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation using Groq\n",
    "def generate_text(prompt: str, max_tokens: int = 500) -> str:\n",
    "    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not groq_api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY is not set\")\n",
    "    groq_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {groq_api_key}\", \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    response = requests.post(groq_url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3323b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    search_query: str\n",
    "    sources: List[Dict[str, Any]]\n",
    "    summary: str\n",
    "    report: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b088ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class PaperState(TypedDict):\n",
    "    topic: str\n",
    "    search_results: List[Dict[str, Any]]\n",
    "    sections: Dict[str, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8bcc5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Research Paper on Transformers in Natural Language Processing\n",
       "\n",
       "## Abstract\n",
       "\n",
       "Transformers have revolutionized the field of Natural Language Processing (NLP) with their novel architecture and exceptional performance. The evolution of Transformers has led to significant advancements in various NLP applications, setting new benchmarks for performance and capability. At its core, the Transformer model consists of an encoder-decoder architecture, which utilizes positional encoding, multi-head attention, and feed-forward networks to process sequential data.\n",
       "\n",
       "The Transformer architecture enables systems to perform complex tasks such as language translation, text summarization, and sentiment analysis with remarkable accuracy. The multi-head attention mechanism allows the model to focus on different aspects of the input data, while the feed-forward networks facilitate the transformation of the input data into meaningful representations.\n",
       "\n",
       "The impact of Transformers on NLP has been profound, with applications ranging from language modeling and text classification to machine translation and question answering. The ability of Transformers to handle sequential data with ease has made them an indispensable tool in the NLP toolkit. Furthermore, the Transformer architecture has also been successfully applied to computer vision tasks, demonstrating its versatility and potential for multi-modal learning.\n",
       "\n",
       "Overall, the Transformer architecture has transformed the landscape of NLP, offering a powerful and flexible framework for modeling complex linguistic phenomena. Its exceptional performance and versatility have made it a widely adopted technique in the field, with ongoing research focused on further refining and extending its capabilities.\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Here is a potential introduction that provides background information on Transformers in Natural Language Processing, states the purpose of this paper, and outlines its structure:\n",
       "\n",
       "The advent of Transformers has marked a significant turning point in the field of Natural Language Processing (NLP), revolutionizing the way machines understand and interact with human language. Since their introduction, Transformers have redefined numerous applications in NLP, establishing new standards of performance and capability [1]. At its core, the Transformer model is a neural network architecture that leverages an encoder-decoder architecture, positional encoding, multi-head attention, and feed-forward networks to process sequential data [3]. This novel architecture has enabled systems to perform complex tasks such as language translation, text summarization, and sentiment analysis with remarkable accuracy [2].\n",
       "\n",
       "The purpose of this paper is to provide an in-depth examination of the Transformer architecture and its impact on the field of NLP. By exploring the fundamental components of the Transformer model and its applications in various NLP tasks, this paper aims to provide a comprehensive understanding of the evolution and significance of Transformers in NLP.\n",
       "\n",
       "This paper is organized as follows. Section 1 provides an introduction to the Transformer architecture and its background in NLP. Section 2 delves into the core components of the Transformer model, including the encoder-decoder architecture, positional encoding, multi-head attention, and feed-forward networks. Section 3 discusses the applications of Transformers in various NLP tasks, including language modeling, text classification, machine translation, and question answering. Section 4 examines the impact of Transformers on the field of NLP and their potential for multi-modal learning. Finally, Section 5 concludes with a summary of the key findings and future directions for research in this area.\n",
       "\n",
       "This introduction provides a background on Transformers in NLP, states the purpose of the paper, and outlines its structure, ensuring consistency with the abstract.\n",
       "\n",
       "## Literature review\n",
       "\n",
       "## Review of Existing Literature on Transformers in Natural Language Processing\n",
       "\n",
       "The Transformer architecture has revolutionized the field of Natural Language Processing (NLP) with its exceptional performance and versatility [1]. Since its introduction, Transformers have redefined numerous applications in NLP, establishing new standards of performance and capability. This review aims to provide an in-depth examination of the existing literature on Transformers in NLP, highlighting key studies and findings.\n",
       "\n",
       "### Fundamental Components of the Transformer Model\n",
       "\n",
       "The Transformer model consists of an encoder-decoder architecture, which utilizes positional encoding, multi-head attention, and feed-forward networks to process sequential data [3]. The multi-head attention mechanism allows the model to focus on different aspects of the input data, while the feed-forward networks facilitate the transformation of the input data into meaningful representations. A deep dive into the core technology of Transformers reveals that the encoder-decoder architecture is a crucial component, enabling the model to handle sequential data with ease [3].\n",
       "\n",
       "### Applications of Transformers in NLP\n",
       "\n",
       "Transformers have been successfully applied to various NLP tasks, including language modeling, text classification, machine translation, and question answering [2]. Their ability to handle sequential data with ease has made them an indispensable tool in the NLP toolkit. The Transformer architecture has also been used for language translation, text summarization, and sentiment analysis with remarkable accuracy [2]. For instance, Transformers have achieved state-of-the-art results in machine translation tasks, outperforming traditional sequence-to-sequence models [2].\n",
       "\n",
       "### Impact of Transformers on NLP\n",
       "\n",
       "The impact of Transformers on NLP has been profound, with applications ranging from language modeling and text classification to machine translation and question answering [1]. The Transformer architecture has transformed the landscape of NLP, offering a powerful and flexible framework for modeling complex linguistic phenomena. Its exceptional performance and versatility have made it a widely adopted technique in the field, with ongoing research focused on further refining and extending its capabilities.\n",
       "\n",
       "### Key Studies and Findings\n",
       "\n",
       "Several studies have demonstrated the effectiveness of Transformers in various NLP tasks. For example, [1] highlights the evolution and impact of Transformers in NLP, while [3] provides a deep dive into the core technology of Transformers. [2] provides an overview of the Transformer architecture and its applications in machine learning, including NLP.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In conclusion, the Transformer architecture has revolutionized the field of NLP with its exceptional performance and versatility. The existing literature highlights the significance of Transformers in various NLP tasks, including language modeling, text classification, machine translation, and question answering. Further research is needed to fully explore the potential of Transformers and to address the challenges associated with their application in NLP.\n",
       "\n",
       "## References\n",
       "\n",
       "[1] biolecta.com. (n.d.). Transformers in NLP: Evolution and Impact. Retrieved from https://biolecta.com/articles/transformers-nlp-evolution-impact/\n",
       "\n",
       "[2] GeeksforGeeks. (n.d.). Transformers in Machine Learning. Retrieved from https://www.geeksforgeeks.org/getting-started-with-transformers/\n",
       "\n",
       "[3] Analytics Vidhya. (2024, April). Understanding Transformers: A Deep Dive into NLP's Technology. Retrieved from https://www.analyticsvidhya.com/blog/2024/04/understanding-transformers-a-deep-dive-into-nlps-core-technology/\n",
       "\n",
       "## Methods\n",
       "\n",
       "## Methodology\n",
       "\n",
       "This review paper aims to provide a comprehensive understanding of the Transformer architecture and its impact on the field of Natural Language Processing (NLP). To achieve this goal, a systematic search of existing literature was conducted.\n",
       "\n",
       "### Search Strategy\n",
       "\n",
       "The search strategy involved querying various online databases and sources, including academic articles, blog posts, and online tutorials. The search terms used included \"Transformers in NLP,\" \"Transformer architecture,\" \"natural language processing,\" and \"deep learning.\" The search was limited to articles and sources that focused on the Transformer architecture and its applications in NLP.\n",
       "\n",
       "### Source Selection\n",
       "\n",
       "The sources selected for this review were chosen based on their relevance to the topic, credibility, and recency. The sources included:\n",
       "\n",
       "1. **biolecta.com**: \"Transformers in NLP: Evolution and Impact\" - This article provides an overview of the evolution and impact of Transformers in NLP.\n",
       "2. **GeeksforGeeks**: \"Transformers in Machine Learning\" - This article provides an introduction to the Transformer architecture and its applications in machine learning, including NLP.\n",
       "3. **Analytics Vidhya**: \"Understanding Transformers: A Deep Dive into NLP's Technology\" - This article provides a detailed explanation of the Transformer model, including its fundamental components and applications in NLP.\n",
       "\n",
       "### Inclusion and Exclusion Criteria\n",
       "\n",
       "The inclusion criteria for this review were:\n",
       "\n",
       "* Sources that focused on the Transformer architecture and its applications in NLP.\n",
       "* Sources that were published in English.\n",
       "* Sources that were published within the last 5 years.\n",
       "\n",
       "The exclusion criteria were:\n",
       "\n",
       "* Sources that did not focus on the Transformer architecture and its applications in NLP.\n",
       "* Sources that were not published in English.\n",
       "* Sources that were published more than 5 years ago.\n",
       "\n",
       "### Data Extraction\n",
       "\n",
       "The data extraction process involved carefully reading and analyzing the selected sources to extract relevant information. The extracted data included:\n",
       "\n",
       "* The fundamental components of the Transformer model.\n",
       "* The applications of Transformers in NLP.\n",
       "* The impact of Transformers on the field of NLP.\n",
       "\n",
       "### Analysis and Synthesis\n",
       "\n",
       "The extracted data were analyzed and synthesized to identify key themes, patterns, and findings. The analysis involved:\n",
       "\n",
       "* Identifying the core components of the Transformer model and their significance in NLP.\n",
       "* Examining the applications of Transformers in various NLP tasks.\n",
       "* Discussing the impact of Transformers on the field of NLP.\n",
       "\n",
       "The findings of this review are presented in the following sections, providing a comprehensive understanding of the Transformer architecture and its significance in NLP.\n",
       "\n",
       "## Results\n",
       "\n",
       "## Main Findings and Implications of Transformers in Natural Language Processing\n",
       "\n",
       "The Transformer architecture has revolutionized the field of Natural Language Processing (NLP) with its exceptional performance and versatility (biolecta.com, n.d.). The evolution of Transformers has led to significant advancements in various NLP applications, setting new benchmarks for performance and capability. This section presents the main findings from the search results, summarizing key concepts and implications related to Transformers in NLP.\n",
       "\n",
       "### Fundamental Components of the Transformer Model\n",
       "\n",
       "The Transformer model consists of an encoder-decoder architecture, which utilizes positional encoding, multi-head attention, and feed-forward networks to process sequential data (Analytics Vidhya, 2024). The multi-head attention mechanism allows the model to focus on different aspects of the input data, while the feed-forward networks facilitate the transformation of the input data into meaningful representations. A deep dive into the core technology of Transformers reveals that the encoder-decoder architecture is a crucial component, enabling the model to handle sequential data with ease (Analytics Vidhya, 2024).\n",
       "\n",
       "### Applications of Transformers in NLP\n",
       "\n",
       "Transformers have been successfully applied to various NLP tasks, including language modeling, text classification, machine translation, and question answering (GeeksforGeeks, n.d.). Their ability to handle sequential data with ease has made them an indispensable tool in the NLP toolkit. The Transformer architecture has also been used for language translation, text summarization, and sentiment analysis with remarkable accuracy (GeeksforGeeks, n.d.).\n",
       "\n",
       "### Impact of Transformers on NLP\n",
       "\n",
       "The impact of Transformers on NLP has been profound, with applications ranging from language modeling and text classification to machine translation and question answering (biolecta.com, n.d.). The Transformer architecture has transformed the landscape of NLP, offering a powerful and flexible framework for modeling complex linguistic phenomena. Its exceptional performance and versatility have made it a widely adopted technique in the field, with ongoing research focused on further refining and extending its capabilities.\n",
       "\n",
       "### Key Implications\n",
       "\n",
       "The Transformer architecture has several key implications for NLP:\n",
       "\n",
       "1. **Improved Performance**: Transformers have achieved state-of-the-art results in various NLP tasks, outperforming traditional sequence-to-sequence models (GeeksforGeeks, n.d.).\n",
       "2. **Flexibility and Versatility**: The Transformer architecture can be applied to a wide range of NLP tasks, including language modeling, text classification, machine translation, and question answering (biolecta.com, n.d.).\n",
       "3. **Efficient Handling of Sequential Data**: The Transformer model can handle sequential data with ease, making it an indispensable tool in the NLP toolkit (Analytics Vidhya, 2024).\n",
       "\n",
       "In conclusion, the Transformer architecture has revolutionized the field of NLP with its exceptional performance and versatility. The existing literature highlights the significance of Transformers in various NLP tasks, including language modeling, text classification, machine translation, and question answering. Further research is needed to fully explore the potential of Transformers and to address the challenges associated with their application in NLP.\n",
       "\n",
       "## References\n",
       "\n",
       "[1] biolecta.com. (n.d.). Transformers in NLP: Evolution and Impact. Retrieved from https://biolecta.com/articles/transformers-nlp-evolution-impact/\n",
       "\n",
       "[2] GeeksforGeeks. (n.d.). Transformers in Machine Learning. Retrieved from https://www.geeksforgeeks.org/getting-started-with-transformers/\n",
       "\n",
       "[3] Analytics Vidhya. (2024, April). Understanding Transformers: A Deep Dive into NLP's Technology. Retrieved from https://www.analyticsvidhya.com/blog/2024/04/understanding-transformers-a-deep-dive-into-nlps-core-technology/\n",
       "\n",
       "## Discussion\n",
       "\n",
       "## Significance of Findings, Implications, and Limitations\n",
       "\n",
       "The findings of this review highlight the significance of the Transformer architecture in Natural Language Processing (NLP). The Transformer model, with its encoder-decoder architecture, positional encoding, multi-head attention, and feed-forward networks, has revolutionized the field of NLP with its exceptional performance and versatility.\n",
       "\n",
       "### Significance of Findings\n",
       "\n",
       "The Transformer architecture has been shown to achieve state-of-the-art results in various NLP tasks, including language modeling, text classification, machine translation, and question answering. The ability of Transformers to handle sequential data with ease has made them an indispensable tool in the NLP toolkit. The findings of this review are consistent with the existing literature, which highlights the significance of Transformers in NLP.\n",
       "\n",
       "### Implications\n",
       "\n",
       "The Transformer architecture has several key implications for NLP:\n",
       "\n",
       "1. **Improved Performance**: Transformers have achieved state-of-the-art results in various NLP tasks, outperforming traditional sequence-to-sequence models.\n",
       "2. **Flexibility and Versatility**: The Transformer architecture can be applied to a wide range of NLP tasks, including language modeling, text classification, machine translation, and question answering.\n",
       "3. **Efficient Handling of Sequential Data**: The Transformer model can handle sequential data with ease, making it an indispensable tool in the NLP toolkit.\n",
       "\n",
       "### Limitations\n",
       "\n",
       "Despite the significance of the findings, there are several limitations to consider:\n",
       "\n",
       "1. **Limited Scope**: This review focused on a limited number of sources, which may not be representative of the entire field of NLP.\n",
       "2. **Lack of Experimental Evaluation**: This review did not involve experimental evaluation of the Transformer architecture, which may limit the validity of the findings.\n",
       "3. **Rapidly Evolving Field**: The field of NLP is rapidly evolving, and new techniques and architectures are being proposed regularly. Therefore, the findings of this review may become outdated soon.\n",
       "\n",
       "### Relation to Introduction and Literature Review\n",
       "\n",
       "The findings of this review are consistent with the introduction and literature review, which highlighted the significance of Transformers in NLP. The literature review provided a comprehensive overview of the existing literature on Transformers in NLP, highlighting key studies and findings. The introduction provided a background on Transformers in NLP, stated the purpose of the paper, and outlined its structure.\n",
       "\n",
       "### Future Directions\n",
       "\n",
       "Future research should focus on:\n",
       "\n",
       "1. **Experimental Evaluation**: Experimental evaluation of the Transformer architecture in various NLP tasks to validate its performance and effectiveness.\n",
       "2. **Extension to Multi-Modal Learning**: Extension of the Transformer architecture to multi-modal learning, including computer vision and speech recognition.\n",
       "3. **Addressing Challenges**: Addressing the challenges associated with the application of Transformers in NLP, including interpretability, explainability, and robustness.\n",
       "\n",
       "In conclusion, the Transformer architecture has revolutionized the field of NLP with its exceptional performance and versatility. The findings of this review highlight the significance of Transformers in various NLP tasks, including language modeling, text classification, machine translation, and question answering. However, there are several limitations to consider, and future research should focus on experimental evaluation, extension to multi-modal learning, and addressing challenges associated with the application of Transformers in NLP.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "## Summary of Main Points\n",
       "\n",
       "The Transformer architecture has revolutionized the field of Natural Language Processing (NLP) with its exceptional performance and versatility. The main points of this review are:\n",
       "\n",
       "1. **Transformer Architecture**: The Transformer model consists of an encoder-decoder architecture, which utilizes positional encoding, multi-head attention, and feed-forward networks to process sequential data.\n",
       "2. **Applications of Transformers in NLP**: Transformers have been successfully applied to various NLP tasks, including language modeling, text classification, machine translation, and question answering.\n",
       "3. **Impact of Transformers on NLP**: The Transformer architecture has transformed the landscape of NLP, offering a powerful and flexible framework for modeling complex linguistic phenomena.\n",
       "4. **Key Implications**: The Transformer architecture has several key implications for NLP, including improved performance, flexibility and versatility, and efficient handling of sequential data.\n",
       "\n",
       "## Future Research Directions\n",
       "\n",
       "Based on the findings of this review, several future research directions are proposed:\n",
       "\n",
       "1. **Experimental Evaluation**: Experimental evaluation of the Transformer architecture in various NLP tasks to validate its performance and effectiveness.\n",
       "2. **Extension to Multi-Modal Learning**: Extension of the Transformer architecture to multi-modal learning, including computer vision and speech recognition.\n",
       "3. **Addressing Challenges**: Addressing the challenges associated with the application of Transformers in NLP, including:\n",
       "\t* **Interpretability**: Developing techniques to interpret and understand the decisions made by Transformers.\n",
       "\t* **Explainability**: Developing techniques to explain the outputs of Transformers.\n",
       "\t* **Robustness**: Improving the robustness of Transformers to adversarial attacks and out-of-distribution data.\n",
       "4. **Investigating the Limitations of Transformers**: Investigating the limitations of Transformers, including their:\n",
       "\t* **Computational Requirements**: Developing techniques to reduce the computational requirements of Transformers.\n",
       "\t* **Data Requirements**: Developing techniques to reduce the data requirements of Transformers.\n",
       "5. **Exploring New Applications**: Exploring new applications of Transformers in NLP, including:\n",
       "\t* **Low-Resource Languages**: Developing Transformers for low-resource languages.\n",
       "\t* **Specialized Domains**: Developing Transformers for specialized domains, such as medicine and law.\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In conclusion, the Transformer architecture has revolutionized the field of NLP with its exceptional performance and versatility. The findings of this review highlight the significance of Transformers in various NLP tasks, including language modeling, text classification, machine translation, and question answering. Future research should focus on experimental evaluation, extension to multi-modal learning, addressing challenges, investigating limitations, and exploring new applications.\n",
       "\n",
       "## References\n",
       "\n",
       "[1] Transformers in NLP: Evolution and Impact - biolecta.com. Retrieved from https://biolecta.com/articles/transformers-nlp-evolution-impact/\n",
       "[2] Transformers in Machine Learning - GeeksforGeeks. Retrieved from https://www.geeksforgeeks.org/getting-started-with-transformers/\n",
       "[3] Understanding Transformers: A Deep Dive into NLP's Technology. Retrieved from https://www.analyticsvidhya.com/blog/2024/04/understanding-transformers-a-deep-dive-into-nlps-core-technology/\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "import os\n",
    "import getpass\n",
    "import requests\n",
    "from typing import Dict, Any, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Set environment variables for API keys\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"GROQ_API_KEY\")\n",
    "\n",
    "# Define the state structure\n",
    "state_schema = {\n",
    "    \"topic\": str,\n",
    "    \"search_results\": List[Dict[str, Any]],\n",
    "    \"sections\": Dict[str, str]\n",
    "}\n",
    "\n",
    "# Define section order and prompts\n",
    "section_order = [\"abstract\", \"introduction\", \"literature_review\", \"methods\", \"results\", \"discussion\", \"conclusion\", \"references\"]\n",
    "\n",
    "section_prompts = {\n",
    "    \"abstract\": \"Generate a concise abstract (150-250 words) summarizing the key points about {topic} based on the provided search results. Include the main findings and conclusions. also dont start with here is the result as i am directly including it in the paper result so directly just give me result\",\n",
    "    \"introduction\": \"Write an introduction providing background information on {topic}, stating the purpose of this paper, and outlining its structure. Ensure consistency with the abstract.\",\n",
    "    \"literature_review\": \"Provide a review of existing literature on {topic} based on the search results. Highlight key studies and findings, using in-text citations like [1], [2], etc.\",\n",
    "    \"methods\": \"Describe the methodology, explaining how the search was conducted (e.g., query and source selection) since this is a review paper.\",\n",
    "    \"results\": \"Present the main findings from the search results, summarizing key concepts and implications related to {topic}. Use in-text citations.\",\n",
    "    \"discussion\": \"Discuss the significance of the findings, their implications, and limitations, relating back to the introduction and literature review.\",\n",
    "    \"conclusion\": \"Summarize the main points and suggest future research directions.\"\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def get_search_results(query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
    "    tavily_api_key = os.environ.get(\"TAVILY_API_KEY\")\n",
    "    if not tavily_api_key:\n",
    "        raise ValueError(\"TAVILY_API_KEY is not set\")\n",
    "    search_url = \"https://api.tavily.com/search\"\n",
    "    search_params = {\n",
    "        \"api_key\": tavily_api_key,\n",
    "        \"query\": query,\n",
    "        \"search_depth\": \"basic\",\n",
    "        \"max_results\": max_results\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(search_url, json=search_params)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"results\", [])\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Search API error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def format_search_results(search_results: List[Dict[str, Any]]) -> str:\n",
    "    formatted = \"\"\n",
    "    for i, result in enumerate(search_results, 1):\n",
    "        title = result.get(\"title\", \"Untitled\")\n",
    "        url = result.get(\"url\", \"No URL\")\n",
    "        content = result.get(\"content\", \"\")[:200]\n",
    "        formatted += f\"Source [{i}]: {title}\\nURL: {url}\\nExcerpt: {content}...\\n\\n\"\n",
    "    return formatted\n",
    "\n",
    "def format_previous_sections(sections: Dict[str, str]) -> str:\n",
    "    formatted = \"\"\n",
    "    for section_name in section_order:\n",
    "        content = sections.get(section_name, \"\")\n",
    "        if content:\n",
    "            formatted += f\"{section_name.replace('_', ' ').capitalize()}:\\n{content}\\n\\n\"\n",
    "    return formatted\n",
    "\n",
    "def generate_section(section_name: str, state: Dict[str, Any]) -> str:\n",
    "    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
    "    if not groq_api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY is not set\")\n",
    "    groq_url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {groq_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    base_prompt = section_prompts[section_name].format(topic=state[\"topic\"])\n",
    "    formatted_results = format_search_results(state[\"search_results\"])\n",
    "    formatted_previous = format_previous_sections(state[\"sections\"])\n",
    "    prompt = f\"{base_prompt}\\n\\nSearch results:\\n{formatted_results}\\n\\nPrevious sections:\\n{formatted_previous}\"\n",
    "    data = {\n",
    "        \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert academic writer. Generate the requested section in a clear, concise, and scholarly style.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.3\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(groq_url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            print(f\"Unexpected response: {result}\")\n",
    "            return \"Error generating section\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"API error: {str(e)}\")\n",
    "        return \"Error generating section\"\n",
    "\n",
    "# Define nodes\n",
    "def search_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    search_query = f\"{state['topic']} explanation concepts applications\"\n",
    "    state[\"search_results\"] = get_search_results(search_query)\n",
    "    return state\n",
    "\n",
    "def section_node(section_name: str):\n",
    "    def node_func(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        section_text = generate_section(section_name, state)\n",
    "        state[\"sections\"][section_name] = section_text\n",
    "        return state\n",
    "    return node_func\n",
    "\n",
    "def references_node(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    references = []\n",
    "    for i, result in enumerate(state[\"search_results\"], 1):\n",
    "        title = result.get(\"title\", \"Untitled\")\n",
    "        url = result.get(\"url\", \"No URL\")\n",
    "        citation = f\"[{i}] {title}. Retrieved from {url}\"\n",
    "        references.append(citation)\n",
    "    state[\"sections\"][\"references\"] = \"\\n\".join(references)\n",
    "    return state\n",
    "\n",
    "\n",
    "# Set up the LangGraph workflow\n",
    "memory = MemorySaver()\n",
    "builder = StateGraph(PaperState)\n",
    "builder.add_node(\"search\", search_node)\n",
    "for section in section_order[:-1]:\n",
    "    builder.add_node(section, section_node(section))\n",
    "builder.add_node(\"references\", references_node)\n",
    "\n",
    "builder.add_edge(\"search\", \"abstract\")\n",
    "current_section = \"abstract\"\n",
    "for next_section in section_order[1:]:\n",
    "    builder.add_edge(current_section, next_section)\n",
    "    current_section = next_section\n",
    "builder.add_edge(\"references\", END)\n",
    "builder.set_entry_point(\"search\")\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Format and display the paper\n",
    "def format_paper(state: Dict[str, Any]) -> str:\n",
    "    paper = f\"# Research Paper on {state['topic']}\\n\\n\"\n",
    "    for section_name in section_order:\n",
    "        content = state[\"sections\"].get(section_name, \"\")\n",
    "        if content:\n",
    "            paper += f\"## {section_name.replace('_', ' ').capitalize()}\\n\\n{content}\\n\\n\"\n",
    "    return paper\n",
    "\n",
    "# Main execution function\n",
    "def run_research(topic: str):\n",
    "    initial_state = {\n",
    "        \"topic\": topic,\n",
    "        \"search_results\": [],\n",
    "        \"sections\": {section: \"\" for section in section_order}\n",
    "    }\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    final_state = graph.invoke(initial_state, config)\n",
    "    paper_md = format_paper(final_state)\n",
    "    display(Markdown(paper_md))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    run_research(\"Transformers in Natural Language Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59dd678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meraenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
